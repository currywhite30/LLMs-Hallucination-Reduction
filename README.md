# LLMs-Uncertainty-Quantification
Exploring various methods to quantify and utilize the uncertainty of LLMs.

## 1. xxx

### (1) xxx
| Year | Title                                                        | **Venue** | **Affiliation** | **Tasks** |                           Brief Conclusions                             |                       Paper                   |                             Code                             |
| :--: | :----------------------------------------------------------- | :-------: | :-------: | :-------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| 2024 | **Detecting hallucinations in large language models using semantic entropy** |    19 June 2024   |    OATML, University of Oxford   |    open-ended question answering   | proposing entropy-based uncertainty estimators for LLMs to detect a subset of hallucinations | [Link](https://www.nature.com/articles/s41586-024-07421-0) |  [Link](https://github.com/jlko/semantic_uncertainty) |
| 2024 | **Large Language Models Must Be Taught to Know What They Don't Know** |    Arxiv 2024   |    New York University, Abacus AI, Cambridge University   |    multiple-choice and open-ended question answering   | Fine-tuning for better uncertainties provides faster and more reliable uncertainty estimates. | [Link](https://arxiv.org/abs/2406.08391) |  [Link](https://github.com/activatedgeek/calibration-tuning) |
