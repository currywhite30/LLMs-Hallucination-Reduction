# LLMs-Hallucination-Reduction
Exploring various methods to mitigate the hallucination of LLMs.

## 1. xxx

### (1) xxx
| Year | Title                                                        | **Venue** | **Affiliation** |                           Brief Conclusions                             |                       Paper                   |                             Code                             |
| :--: | :----------------------------------------------------------- | :-------: | :-------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| 2024 | **Large Language Models Must Be Taught to Know What They Don't Know** |    Arxiv 2024   |    New York University, Abacus AI, Cambridge University   | 1. Fine-tuning for better uncertainties provides faster and more reliable uncertainty estimates. <br> 2.Provide a guide to teaching language models to know what they don't know using a calibration dataset. <br> 3.Models can be used not just to estimate their own uncertainties but also the uncertainties of other models. | [Link](https://arxiv.org/abs/2406.08391) |  [Link](https://github.com/activatedgeek/calibration-tuning) |
